## 0.2 Methodology II

### **Pipeline**
- **Definition**: The term pipeline describes a sequence of operations. A supervised machine learning pipeline is the sequence of operations that produce a prediction (output) using a set of predictors (input).
- A pipeline can consist of multiple stages, including:
  - **Transformation stages** (where input data is processed).
  - **Several machine learning models running in parallel**.
  - **A final aggregation stage** (where individual outputs are combined to produce one single output).
- The stages in a pipeline can be either hand-crafted or trained.
- After training, the parameters of a pipeline remain fixed.
- Pipelines can be tested and deployed.
- The quality of the prediction depends on **all the pipeline stages.**

### **Normalization**
- **Definition**: Data normalization is a tunable transformation stage that allows us to scale attributes so that their values belong to similar ranges.
- It helps to ensure that attributes with different units, dimensions, or dynamic ranges do not disproportionately influence the model.
- **Min-max normalization**:
  - **Definition**: produces values within the same continuous range [0, 1].
  - **Formula**: $z = \frac{x - \min(x)}{\max(x) - \min(x)}$
- **Standardization**:
  - **Definition**: produces values with 0 mean and unit standard deviation.
  - **Formula**: $z = \frac{x - \mu}{\sigma}$, where $\mu$ is the average of $x$ and $\sigma$ is its standard deviation.
- **Observations**:
  - Outliers can have a negative impact on normalization.
  - Non-linear scaling options exist, such as softmax scaling and logarithmic scaling.
  - The original values in your dataset might be relevant, so consider unintended effects and distortions.
  - Many machine learning algorithms might produce different solutions after scaling.

### **Transformations**
- **Definition**: Transformations are data manipulations that change the way that we represent our samples. They can be seen as moving samples from one space (original space) to another (destination space).
- **Types**:
  - **Linear transformations**: can be seen as a rotation and scaling.
  - **Non-linear transformations**: have no unique description.
- **Dimensionality reduction**: If the destination space has fewer dimensions than the original one.
  - **Feature selection**: the destination space is defined by a subset of the original attributes.
    - **Filtering**: consider each attribute individually.
    - **Wrapping**: evaluate different subsets of features together.
  - **Feature extraction**: the new attributes are defined as operations on the original attributes.
- **Complex models and kernel methods**:
  - Many complex machine learning models can be interpreted as a transformation followed by a simple model.
  - Kernel methods, such as support vector machines, implicitly define such transformations using so-called kernel functions.

### **Ensembles**
- **Definition**: An ensemble is a machine learning strategy that combines the output from individual models (base models).
- **Principles**:
  - Base models need to be as diverse as possible.
  - Diversity can be achieved by training:
    - A family of models with random subsets of the data.
    - Different models with random subsets of attributes.
    - Different families of models altogether.
- **Bagging (Bootstrap Aggregating)**:
  - **Definition**: generates K sub-datasets by **bootstrapping** and trains K simple base models with each sub-dataset.
  - The final model combines the predictions of the base models by averaging or voting.
- **Decision trees**:
  - **Definition**: partition the predictor space into multiple decision regions by implementing sequences of splitting rules using one predictor only.
  - The goal is to create pure leaves, i.e. containing as many samples from the same class as possible.
  - Decision trees are built recursively.
  - Splits are axis-parallel (decisions using one predictor).
  - The chosen split is such that the purity of the resulting regions is higher than any other split.
  - We stop when a given criterion is met, such as the number of samples in a region.
- **Random forests**:
  - **Definition**: an ensemble of decision trees.
  - Random forests train many individual trees by randomising the training samples and the predictors.
  - Predictions are obtained by averaging the individual predictions.
- **Boosting**:
  - **Definition**: generates a sequence of simple base models, where each successive model focuses on the samples that the previous models could not handle properly.
  - Each new base model generated by boosting depends on the previous models.

### **Summary**
- **Deploying machine learning pipelines**:
  - Pipelines define sequences of operations on data.
  - In machine learning we deploy pipelines, not just models.
  - Complex machine learning models can be seen as pipelines.
- **Transformations**:
  - The purpose is to use a more convenient representation for our data.
  - Normalisations are transformations where each attribute is scaled individually.
  - In feature selection we select a subset of attributes, whereas in feature extraction we produce a new set of attributes by processing the input attributes.
- **Ensembles**:
  - Combine the output from individual models.
  - The idea is that each model learns different aspects of the true underlying model.
  - Can be created by using different subsets of samples, different subsets of attributes or different families of models.