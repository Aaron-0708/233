## 2.1 Structure Analysis

### **Cluster analysis**

- **Definition**: Clustering is a family of unsupervised learning algorithms that describe the structure of a dataset as groups, or clusters, of similar samples.
- A notion of **similarity** is needed in order for us to partition a dataset into clusters.

**Similarity as proximity**

- Clusters can be defined as groups of samples that are **close** to one another. In this case, we use proximity as our notion of similarity.

- Mathematically, there are different ways of defining a distance. Given two samples $\mathbf{x}_i$ and $\mathbf{x}_j$ consisting of $P$ attributes, $x_{i,1}$, ..., $x_{i,P}$ and $x_{j,1}$, ..., $x_{j,P}$, the **squared distance** $d_{i,j}$ is defined as

  $$
  d_{i,j} = (x_{i,1} - x_{j,1})^2 + \cdots + (x_{i,P} - x_{j,P})^2
  $$

**A proximity-based quality metric**

Using distance as our notion of similarity, samples within the same cluster should be close to one another and samples from different clusters should be far apart.

Assume we have two clusters $C_0$ and $C_1$. The **intra-cluster** sample scatter $I(C_0)$ and $I(C_1)$ is the sum of the square distances between samples in the same cluster:
$$
I(C_0) = \frac{1}{2}\sum_{\mathbf{x}_i, \mathbf{x}_j \text{ in } C_0} d_{i,j}, \quad
I(C_1) = \frac{1}{2}\sum_{\mathbf{x}_i, \mathbf{x}_j \text{ in } C_1} d_{i,j}
$$
and the **inter-cluster** sample scatter $O(C_0, C_1)$ is defined as the sum of the distances between samples in different clusters:
$$
O(C_0, C_1) = \sum_{\mathbf{x}_i \text{ in } C_0, \mathbf{x}_j \text{ in } C_1} d_{i,j}
$$
The **best** clustering arrangement has the **lowest intra-cluster** sample scatter and **highest inter-cluster** sample scatter. We can show that reducing the intra-cluster scatter increases the inter-cluster scatter!

The intra-cluster and inter-cluster sample scatters allow us to compare and rank different clustering arrangements.

The question is, can we create an algorithm capable of automatically identifying the **best clustering** arrangement? This is an **optimisation** question.

### **K-means clustering: Prototypes**

- **Definition**: A simple way to describe a cluster is by using **cluster prototypes**, such as the centre of a cluster.

- Given a cluster $C_0$ consisting of $N_0$ samples, its centre (or mean) $\mu_0$ can be calculated as:
  $$
  \mu_0 = \frac{1}{N_0} \sum_{\mathbf{x}_i \text{ in } C_0} \mathbf{x}_i
  $$

- Interestingly, the intra-cluster sample scatter can be calculated using the distance between each sample and the cluster prototype $d_i$:
  $$
  I(C_0) = N_0 \sum_{\mathbf{x}_i \text{ in } C_0} d_i
  $$

- Therefore, our notion of clustering quality can be expressed as follows: in a good clustering arrangement, samples are **close to their prototype**.

**K-means clustering**

K-means partitions a dataset into $K$ clusters represented by their **mean** and proceeds iteratively as follows:

- Prototypes are obtained as the centre (or mean) of each cluster.
- Samples are re-assigned to the cluster with the closest prototype.

As the K-means algorithm proceeds, we will see samples been reassigned to different clusters until at some point we reach a stable solution, where no sample is reassigned.

The final solution is a **local optimum**, not necessarily the global one.

**How many clusters?**

K-means requires that we specify the number of clusters $K$ that we want to partition our dataset into.

A natural question is, what's the right number of clusters? The answer to this question depends on the application:

- In some cases we are given the number of clusters (e.g. t-shirt sizes).
- In a discovery scenario we want to find the underlying structure and the number of clusters is unknown.

**Validation** strategies can suggest a suitable value for the hyperparameter $K$. Choosing the value of $K$ producing the lowest $I(C_0)$ would not work however, as $I(C_0)$ always decreases as the number of clusters increase.

**The elbow method**

Assume the true number of clusters is $K_T$. For $K > K_T$, we should expect the increase in quality to be slower than for $K < K_T$, as we will be splitting true clusters.

The true number of clusters can be identified by observing the value of $K$ beyond which the improvement slows down.

### **Density-based clustering: DBSCAN**

In a **non-convex** cluster, we can reach any sample by taking small jumps from sample to sample.

Non-convex scenarios suggest a different notion of cluster as group of samples that are **connected**, rather than simply close: *if I am similar to you, and you are similar to them, I am similar to them too*.

This notion of cluster as a group of connected samples is behind many clustering algorithms, such as DBSCAN (**density-based spatial clustering of applications with noise**).

DBSCAN belongs to the family of **density-based** algorithms, where an estimation of the density of samples around each sample is used to partition the dataset into clusters.

**DBSCAN**

DBSCAN defines two quantities, a **radius** $r$ and a **threshold** $t$. A density is first calculated as the number of samples in a neighbourhood of radius $r$ around each sample (excluding itself). Then, three types of samples are identified:

- **Core**: its density is equal or higher than the threshold $t$.
- **Border**: its density is lower than the threshold $t$, but contains a core sample within its neighbourhood.
- **Outlier**: Any other sample.

**The DBSCAN algorithm proceeds as follows:**

- Identify core, border and outlier samples.
- Pair of core samples that are within each other's neighbourhood are connected. Connected core samples form the **backbone** of a cluster.
- Border samples are assigned to the cluster that has more core samples in the neighbourhood of the border sample.
- Outlier samples are not assigned to any cluster.

### **Hierarchical clustering**

Given a dataset consisting of $N$ samples, there exist two **trivial** clustering solutions: **one single cluster** that includes all the samples, and the solution where **each sample is a cluster** on its own.

K-means produces $K$ clusters, but we need to choose $K$ within $1 \le K \le N$. In DBSCAN clusters are discovered automatically, but the final number of clusters depends on the values of the radius $r$ and the threshold value $t$.

This ambiguity ultimately reveals that **the structure of a dataset can be explored at different levels that expose different properties**.

Hierarchical clustering is a family of clustering approaches that proceed by progressively building clustering arrangements at **different levels**.

The resulting collection of clustering arrangements is hierarchical in the sense that a cluster in one level contains all the samples from one or more clusters in the level below.

The representation of the relationship between clusters at different levels is called a **dendrogram**. At the bottom we find the arrangement where each sample is one cluster and at the top, the whole dataset.

There exist two basic strategies to build a dendrogram:

- The **divisive** or top-down approach splits clusters starting from the top of the dendrogram and stops at the bottom level.
- The **agglomerative** or bottom-up merges two clusters, starting from the bottom until we reach the top level.

There are different options to decide which clusters to merge or split at each level. Common strategies in agglomerative clustering include:

- **Single linkage**: uses the distance between the two closest samples from two clusters. This option results in clusters of arbitrary shapes.
- **Complete linkage**: uses the distance between the two further samples from each pair of clusters. This choice produces clusters that tend to have a spherical shape.
- **Group average**: uses the average distance between samples in two cluster and also produces spherical shapes, although they are more robust to outliers.

### **Summary**

- Unsupervised learning provides with answers for the basic question **where is my data?** (in the attribute space)
- Our answer is a mathematical/computer model. This model can tell us where in the space we have samples (clustering) or the probability to find a sample in a region within the space (density estimation).
- Sometimes we say that our data is unlabelled. What we **really** mean is that we don't treat any attribute as a label that we want to predict. Datasets are neither labelled nor unlabelled.
- Lacking such a target as a label means that **our quality metric is not obvious**.

**Clustering**

- **K-means** is a prototype-based clustering that produces spherical clusters where $K$ is a hyperparameter that has to be set.
- **DBSCAN** is a density-based option suitable for non-convex scenarios and does not require specifying the number of clusters. We need to set $r$ and $t$ and they determine the final number of clusters.
- **Hierarchical clustering** allows to explore the structure of a dataset at multiple levels.

**Comparing clustering solutions**

- We could consider **comparing** the solutions from two different algorithms. However, if they use different definitions of clustering quality, this comparisons will make little sense.
- Clustering is ultimately implemented with an **application** in mind so we should create a final notion of clustering quality based on the specific goals of the application.

**Component analysis**
Component analysis allows us to identify the **directions in the space** that our data are aligned with. This can be useful to transform our dataset, clean it and reduce its dimensionality.